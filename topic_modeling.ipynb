{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Topic Modeling\n",
    "\n",
    "### Sources\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldaseqmodel.html\n",
    "\n",
    "https://markroxor.github.io/gensim/static/notebooks/ldaseqmodel.html\n",
    "\n",
    "https://www.youtube.com/watch?v=7BMsuyBPx90 <-- Dave Blei's Google talk on Dynamic Topic Modelling\n",
    "\n",
    "https://towardsdatascience.com/exploring-the-un-general-debates-with-dynamic-topic-models-72dc0e307696 \n",
    "\n",
    "^ Explains why each new paragraph should be treated as a separate document in DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_DICTIONARY = True\n",
    "LOAD_LDA = True\n",
    "LOAD_DTM = True\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akio\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Akio\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.CSRRD7HKRKC3T3YXA7VY7TAZGLSWDKW6.gfortran-win_amd64.dll\n",
      "C:\\Users\\Akio\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "C:\\Users\\Akio\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Akio\\Anaconda3\\lib\\site-packages\\nltk\\decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.models import ldaseqmodel, ldamodel\n",
    "from gensim.models.wrappers.dtmmodel import DtmModel\n",
    "from gensim.test.utils import datapath\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_para(raw, raw_parser, len_list):\n",
    "    \"\"\"\n",
    "    Input is lowercased, and split by paragraph breaks\n",
    "    using regex. The number of paragraphs are analyzed\n",
    "    and appended in input list, 'len_list'\n",
    "    \n",
    "    Returns a list, 'list_of_paragraphs', which is the\n",
    "    output from the split function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first : string\n",
    "        text of the PDF file\n",
    "    second : bool\n",
    "        whehther to read PDF or txt. Not\n",
    "        currently implemented. \n",
    "    third : list\n",
    "        list to keep track of number of paragraph \n",
    "        extracted from input, 'raw'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "\n",
    "    \"\"\"\n",
    "    lowered = raw.lower()\n",
    "    list_of_paragraphs = re.split(r'\\.[ ][\\n]+', lowered)\n",
    "    len_list.append(len(list_of_paragraphs))\n",
    "    return list_of_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beeper():\n",
    "    '''\n",
    "    Beeps when activated\n",
    "    '''\n",
    "    \n",
    "    eighth = 250\n",
    "    half = 1000\n",
    "    g = 392 #hz\n",
    "    ef = 311 #hz\n",
    "\n",
    "    for i in range(3):\n",
    "        winsound.Beep(g, eighth)\n",
    "    winsound.Beep(ef, half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get/Set common_corpus and common_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Dictionary, corpus, and len documents\n",
      "Wall time: 99.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if LOAD_DICTIONARY:\n",
    "    print(\"loading Dictionary, corpus, and len documents\")\n",
    "    common_dictionary = Dictionary.load('model/common_dictionary')\n",
    "    with open('model/common_corpus_pickled', 'rb') as f:\n",
    "        common_corpus = pickle.load(f)\n",
    "    with open('model/len_docs_pickled', 'rb') as f:\n",
    "        lengths_of_docs = pickle.load(f)\n",
    "    \n",
    "else:\n",
    "    print(\"Dictionary, corpus, and len documents not found; initializing\")\n",
    "\n",
    "    list_of_string = []\n",
    "    list_of_list_of_string = []\n",
    "    lengths_of_docs = []\n",
    "\n",
    "    path = \"data/\"\n",
    "    dirs = os.listdir(path)\n",
    "    for each_pdf in dirs:\n",
    "        print(each_pdf)\n",
    "        with open('txt/{}.txt'.format(str(each_pdf)), 'r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            list_of_paragraphs = extract_para(text, False, lengths_of_docs)\n",
    "            for i in list_of_paragraphs:\n",
    "                list_of_list_of_string.append(preprocess(i, False))\n",
    "\n",
    "    # Create a corpus from a list of texts\n",
    "    common_dictionary = Dictionary(list_of_list_of_string)\n",
    "    common_corpus = [common_dictionary.doc2bow(text) for text in list_of_list_of_string]\n",
    "\n",
    "    common_dictionary.save('model/common_dictionary')\n",
    "    with open('model/common_corpus_pickled', 'wb') as f:\n",
    "        pickle.dump(common_corpus, f)\n",
    "    with open('model/len_docs_pickled', 'wb') as f:\n",
    "        pickle.dump(lengths_of_docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf = models.Nmf(common_corpus, num_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading LDAmodel\n",
      "Wall time: 25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "home = os.getcwd()\n",
    "\n",
    "if LOAD_LDA:\n",
    "    print(\"loading LDAmodel\")\n",
    "    lda = ldamodel.LdaModel.load(os.path.join(home, 'model/LDAmodel_{}'.format(num_topics)))\n",
    "else:\n",
    "    print(\"LDAmodel not found; initializing ldamodel\")\n",
    "    lda = ldamodel.LdaModel(corpus=common_corpus, id2word=common_dictionary, num_topics=num_topics, update_every=1, passes=1)\n",
    "    \n",
    "    print(\"saving\")\n",
    "    lda.save(os.path.join(home, 'model/LDAmodel_{}'.format(num_topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014*\"people\" + 0.010*\"number\" + 0.009*\"total\" + 0.007*\"disaster\" + 0.007*\"refugee\" + 0.007*\"reported\" + 0.006*\"report\" + 0.005*\"world\" + 0.005*\"affected\" + 0.004*\"country\"\n",
      "\n",
      "0.030*\"disaster\" + 0.012*\"society\" + 0.011*\"tel\" + 0.010*\"mail\" + 0.010*\"fax\" + 0.009*\"world\" + 0.007*\"development\" + 0.006*\"see\" + 0.006*\"crescent\" + 0.006*\"risk\"\n",
      "\n",
      "0.014*\"disaster\" + 0.011*\"people\" + 0.009*\"humanitarian\" + 0.009*\"community\" + 0.008*\"warning\" + 0.007*\"early\" + 0.005*\"need\" + 0.005*\"action\" + 0.005*\"data\" + 0.005*\"information\"\n",
      "\n",
      "0.015*\"www\" + 0.012*\"available\" + 0.011*\"disaster\" + 0.010*\"world\" + 0.010*\"org\" + 0.008*\"http\" + 0.007*\"report\" + 0.007*\"online\" + 0.006*\"humanitarian\" + 0.006*\"health\"\n",
      "\n",
      "0.030*\"see\" + 0.018*\"box\" + 0.012*\"humanitarian\" + 0.009*\"society\" + 0.008*\"http\" + 0.008*\"www\" + 0.008*\"org\" + 0.007*\"crescent\" + 0.007*\"fax\" + 0.006*\"tel\"\n",
      "\n",
      "0.013*\"people\" + 0.013*\"per\" + 0.012*\"cent\" + 0.011*\"disaster\" + 0.009*\"aid\" + 0.008*\"number\" + 0.008*\"total\" + 0.008*\"reported\" + 0.007*\"country\" + 0.007*\"world\"\n",
      "\n",
      "0.010*\"disaster\" + 0.007*\"child\" + 0.007*\"risk\" + 0.005*\"world\" + 0.005*\"hiv\" + 0.005*\"community\" + 0.004*\"many\" + 0.004*\"health\" + 0.004*\"humanitarian\" + 0.004*\"aid\"\n",
      "\n",
      "0.012*\"disaster\" + 0.008*\"risk\" + 0.008*\"climate\" + 0.006*\"people\" + 0.005*\"chap\" + 0.005*\"change\" + 0.005*\"humanitarian\" + 0.005*\"food\" + 0.004*\"community\" + 0.004*\"emergency\"\n",
      "\n",
      "0.017*\"disaster\" + 0.009*\"data\" + 0.008*\"risk\" + 0.008*\"humanitarian\" + 0.006*\"report\" + 0.006*\"technology\" + 0.006*\"world\" + 0.006*\"information\" + 0.005*\"ill\" + 0.005*\"response\"\n",
      "\n",
      "0.046*\"hiv\" + 0.013*\"aid\" + 0.013*\"www\" + 0.012*\"health\" + 0.012*\"org\" + 0.010*\"available\" + 0.008*\"disaster\" + 0.006*\"http\" + 0.006*\"online\" + 0.006*\"unhcr\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(\"{}\\n\".format(i[1])) for i in lda.print_topics(num_topics=20, num_words=10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading DTMmodel\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "home = os.getcwd()\n",
    "\n",
    "if LOAD_DTM:\n",
    "    print(\"loading DTMmodel\")\n",
    "    ldaseq = ldaseqmodel.LdaSeqModel.load('model/DTMmodel_{}'.format(num_topics))\n",
    "else:\n",
    "    print(\"DTMmodel not found; initializing DTMmodel\")\n",
    "    bc = bleicorpus.BleiCorpus.serialize(fname=os.path.join(home, 'model/blei_{}'.format(num_topics)), corpus=common_corpus)\n",
    "    ldaseq = ldaseqmodel.LdaSeqModel(corpus=common_corpus, id2word=common_dictionary, time_slice=lengths_of_docs, num_topics=num_topics)\n",
    "    # ldaseq.print_topics(time=0)\n",
    "    \n",
    "    print(\"saving\")\n",
    "    ldaseq.save(os.path.join(home, 'model/DTMmodel_{}'.format(num_topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_dtm_csv():\n",
    "    '''\n",
    "    Saves output of DTM as csv\n",
    "    '''\n",
    "    with open('dtm_output.csv', 'w', newline='') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(['topic', 'time', 'term', 'topic_importance'])\n",
    "        for t in range(len(lengths_of_docs)):\n",
    "            counter = 0\n",
    "            for i in ldaseq.print_topics(time=t, top_terms=15):\n",
    "        #         print(i)\n",
    "                counter += 1\n",
    "                for j in range(len(i)):\n",
    "                    csv_writer.writerow([counter,t,i[j][0], i[j][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_viz():\n",
    "    doc_topic, topic_term, doc_lengths, term_frequency, vocab = ldaseq.dtm_vis(time=0, corpus = common_corpus)\n",
    "    vis_wrapper = pyLDAvis.prepare(topic_term_dists=topic_term, doc_topic_dists=doc_topic, doc_lengths=doc_lengths, vocab=vocab, term_frequency=term_frequency)\n",
    "    pyLDAvis.display(vis_wrapper)\n",
    "    pyLDAvis.save_html(vis_wrapper, 'viz/DTM{}_viz.html'.format(num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda = 1 results in the familiar ranking of terms in decreasing order of their topic-specific probability, and setting lambda = 0 ranks terms solely by their lift\n",
    "\n",
    "lift: the ratio of a termâ€™s probability within a topic to its marginal probability across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_specific_topic(term = 'hiv'):\n",
    "    val_list = []\n",
    "    for i in ldaseq.print_topic_times(topic=0, top_terms=100):\n",
    "        for each_tup in i:\n",
    "            if each_tup[0] == term:\n",
    "                val_list.append(each_tup[1])\n",
    "            else: pass\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    # plt.title('Scores by group and gender')\n",
    "    plt.plot(val_list)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
